<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Wanshui Gan</title>
  
  <meta name="author" content="Chao Ning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chao Ning</name>
              </p>
              <p>
              Ph.D. student from the <a href="https://www.ms.k.u-tokyo.ac.jp/members.html">Sugiyama-Yokoya-Ishida Lab at the University of Tokyo</a>,  Department of Complexity Science and Engineering, advised by Prof. <a href="https://naotoyokoya.com/">Naoto YOKOYA</a>. I am a part-time worker of the <a href="https://geoinformatics2018.com/"> Geoinformatics Team</a> at the RIKEN Center for Advanced Intelligence Project (AIP).
              </p>  

              <p style="text-align:center">
                <a href="mailto:u2j472no6@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=VrRxak8AAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/2J472no"> Github </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:80%;max-width:100%" alt="profile photo" src="images/profile.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Experiences</heading>
            <p>
	      <li style="margin: 5px;" >
                <b>2023-10 --> now:</b> Part-time worker, Geoinformatics Team, RIKEN Center for Advanced Intelligence Project (AIP).
              <li style="margin: 5px;" >
                <b>2023-10 --> now:</b> Student of SpringGX IIW Program, University of Tokyo.
              </li>
             <li style="margin: 5px;" >
                <b>2023-10 --> now:</b> Ph.D. student, Sugiyama-Yokoya-Ishida Lab, Department of Complexity Science and Engineering, University of Tokyo, advised by Prof. Naoto YOKOYA。
              </li>
            <li style="margin: 5px;" >
                <b>2020-09 --> 2023-06:</b> M.S. student, Northwestern Polytechnical University (China). Topics: baseline models; semantic segmentation; monocular depth estimation.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading> Selected Publications </heading>
              <!-- <p>
                * indicates equal contribution
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="height: 50%; width:100%;" src="images/xNet.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Is Pre-training Applicable to the Decoder for Dense Prediction?</papertitle>
              <br>
              <strong>Chao Ning</strong>, Wanshui Gan, Weihao Xuan, Naoto Yokoya
              <!-- <br>
              IROS, 2025
              <br> -->
              <a href="https://arxiv.org/abs/2503.07637">[Paper]</a>
              <br>
              <p> We present ×Net, a simple framework that enables collaboration between a pretrained encoder and a pretrained decoder by directly leveraging pretrained models in the decoder, delivering state of the art results on dense prediction tasks such as monocular depth estimation and semantic segmentation without task specific decoding structures.</p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="height: 25%; width:100%;" src="images/LRDepth.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>LR²Depth: Large-Region Aggregation at Low Resolution for Efficient Monocular Depth Estimation</papertitle>
              <br>
              <strong>Chao Ning</strong>, Weihao Xuan, Wanshui Gan, Naoto Yokoya
              <br>
              IROS, 2025
              <br>
              <a href="https://2J472no.github.io/LRDepth">[Project]</a>
              <br>
              <p> We introduce LR2Depth, a monocular depth estimation method that applies large kernel convolution on low resolution feature maps to aggregate large region context efficiently, achieving state of the art accuracy on NYU Depth v2, KITTI, and SUN RGB D while running about twice as fast as prior methods.</p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/TrapAttention.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Trap attention: Monocular depth estimation with manual traps</papertitle>
              <br>
              <strong>Chao Ning</strong>, Hongping Gan
              <br>
              CVPR, 2023
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10204801">[Paper]</a> <a href="https://github.com/ICSResearch/TrapAttention">[Code]</a>
                         <br>
              <p> We propose a linear complexity trap attention for monocular depth estimation from a single image that uses depthwise convolution to capture long range context within a ViT encoder and decoder, achieving state of the art results on NYU v2 and KITTI with far fewer parameters. </p>
            </td>
          </tr>

            <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SS-ViT.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SS ViT: Observing pathologies of multi-layer perceptron weights and re-setting vision transformer</papertitle>
              <br>
              <strong>Chao Ning</strong>, Hongping Gan
              <br>
              Pattern Recognition, 2025
              <br>
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320325000822">[Paper]</a> <a href="https://github.com/ICSResearch/SS">[Code]</a>
              <br>
              <p> We introduce a lightweight DFC module with two grouped linear layers to learn MLP expansion and activation representations in Vision Transformers, uncover depth dependent weight pathologies, derive depth specific block settings, and demonstrate consistent accuracy and efficiency gains across architectures, for example on ImageNet 1k the PVTv2 model improves accuracy by 0.8 percent while running about 12.9 percent faster with 25 percent fewer FLOPs. </p>
            </td>
          </tr>

           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/LP.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning-based padding: From connectivity on data borders to data padding</papertitle>
              <br>
              <strong>Chao Ning</strong>, Hongping Gan, Minghe Shen, Tao Zhang
              <br>
              Engineering Applications of Artificial Intelligence, 2023
              <br>
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0952197623002324">[Paper]</a> <a href="https://github.com/ICSResearch/LP">[Code]</a>
              <br>
              <p> Padding is employed vastly in convolutional neural networks (CNNs) to maintain the output size. In current padding schemes, the input feature maps are padded using quite simple strategies, e.g., constant zero values in zero padding, which is the most common padding choice. In this work, we propose to use learning-based paradigms to calculate the padding data from the connectivity of the input images on the data borders. Two different modules, i.e., learning-based padding by convolution (LPC) and learning-based padding by attention (LPA), are designed to obtain the padding data from the interdependencies of the image border data along the channel and spatial dimensions, respectively. The designed LPC or LPA is formulated as a generic, plug-and-play unit, which can be a direct replacement for the conventional padding technique. Extensive experiments on image classification and semantic segmentation tasks show that the proposed padding schemes can consistently obtain higher accuracy than standard padding schemes, in various deep network backbones. </p>
            </td>
          </tr>


        </table>

          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> SpringGX scholarship (2023-2026) </li>
              </p>
            </td>
          </tr>
        </tbody></table> 

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> IEEE TVCG, IEEE TIV, IEEE TCSVT, Neurocomputing
              </li>
            </p>
          </td>
        </tr>
      </tbody></table> -->
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=L9EQlZkj5iCdWBMkJkgi98zY_ACS8WXMtmi-BflmwK8"></script>
	  </div>        
	  <br>
	    &copy; ChaoNing | Last updated: November 6, 2025
</center></p>
</body>

</html>
